---
title: "Projek UAS"
author: "Anak Agung Ayu Diva Shanty Darmawan"
date: '2022-04-18'
output: 
  html_document:
    code_folding: show
    number_sections: yes
    toc: yes
    toc_float: yes
---

Sebelum memulai untuk simulasi data, berikut adalah beberapa library yang akan digunakan dalam projek ini.

```{r}
rm(list=ls())
library(readr)
library(gam)
library(foreach)
library(splines)
library(ggplot2)
library(caret)
library(randomForest)
library(ggpubr)
library(gbm)
library(car)
```

Sebelum memulai, juga akan dihapus data-data dari rstudio agar tidak tercampur dengan projek lainnya. Data akan dihapus menggunakan function rm(list = ls()).

# Data Import

Data yang akan digunakan adalah data penjualan rumah di Melbourne yang berjumlah 5000 sekian data. Pertama, data akan di import ke dalam rstudio.

```{r}
melb = read.csv("melb_data_clean.csv")
head(melb)
```

Dengan function read.csv(), data di import dari komputer dan disimpan di dalam variable melb. Dapat dilihat beberapa data awal yang disimpan di dalam melb dari hasil function head(). 

Sesuai dengan perintah yang diberikan untuk projek ini, kita akan menghapus beberapa variable yang ada di dalam melb yang tidak akan digunakan.

```{r}
names(melb)
drop = c("Suburb", "Address", "Method", "SellerG", "Date", "Postcode", "Bedroom2", "Bathroom", "YearBuilt", "Lattitude", "Longtitude", "Propertycount", "SalesYear")
melb2 = melb[, !(names(melb) %in% drop)]
head(melb2)
```

Dengan fungsi names(), dengan mudah kita mengetahui nama-nama kolom variable yang tersimpan di dalam melb. Nama-nama tersebut akan kita simpan di variable drop, dengan fungsi c() untuk menyatukan mereka. Variable yang tersisa dari melb akan disimpan di dalam melb2 dan dari function head() dapat kita lihat beberapa data awal dari melb2.

```{r}
summary(melb2)
```

Dengan function summary(), dapat dilihat ringkasan dari variable-variable yang ada di dalam melb2. Ringkasan ini berbentuk range statistik dari quartil 1, min hingga max data.

# Data Exploration

Pada bagian ini, akan dijelaskan mengenai dependent variable dan independent variable yang akan digunakan dalam projek ini.

## Dependent Variable

Dependent Variable adalah variable yang akan menjadi variable yang kita lihat untuk mengukur apakah berhasil modelnya atau tidak. Anggaplah dependent variable seperti efeknya dari model yang ingin kita simulasikan. Variable yang kita gunakan sebagai dependent variable adalah variable Price. 

```{r}
summary(melb2$Price)
```

Dengan function summary(), kita dapat melihat secara lebih ringkas bagaimana bentuk dari variable Price.

```{r}
hist(melb2$Price) # data skewed ke kiri
boxplot(melb2$Price)
```

Plot-plot di atas membantu kita untuk memvisualisasikan bagaimana bentuk variable price. Dapat dilihat dari kedua plot di atas, bahwa data variable Price telihat skewed, lebih spesifiknya strongly left skewed apabila dilihat dari plot histogram nya.

```{r}
ggplot(melb2) + geom_boxplot(aes(y = Price))
```

Dengan function di atas, kita dapat melihat visualisasi variable Price dengan data melb2. Dari plot yang dihasilkan, terlihat beberapa outliers yang ada dalam variable Price. 

```{r}
length(boxplot.stats(melb2$Price, coef = 3)$out)
```

Dengan function length() kita dapat mencari berapa banyak outliers yang ada di dalam variable Price. Hasilnya adalah 55, sehingga terdapat 55 outliers yang ada di dalam variable Price.

```{r}
out = boxplot.stats(melb2$Price, coef = 3)$out
out_key = which(melb2$Price %in% c(out))
melb2 = melb2[-out_key, ]
```

Dengan kumpulan fungsi di atas, dapat kita hapus outliers yang ada. Function which() bertugas untuk memilah data outliers, yang disimpan di dalam variable out, di dalam melb2 khususnya variable Price. Kemudian outliers ini dihapus dengan function melb[-out_key, ] dan disimpan kembali data yang benar di dalam melb2.

```{r}
ggplot(melb2) + geom_boxplot(aes(y = Price)) 
```

Berikut adalah plot visualisasi dari variable Price dengan melb2 setelah dihapus outliers yang ada. Dapat dilihat bahwa datanya terlihat lebih rapih dan baik. Dapat dilihat juga dari histogram di bawah, datanya menjadi terlihat tidak strongly skewed.

```{r}
ggplot(melb2) + geom_histogram(aes(x = Price))
```


## Independent Variable

Apabila dependent variable adalah efek, maka independent variable adalah 
penyebab dari efek tersebut. Independent variable digunakan sebagai variable yang akan mempengaruhi bagaimana hasil model yang akan dihasilkan. Independent variable yang digunakan pada kali ini adalah: Type, Car, CouncilArea, Regionname, ID, Rooms, Distance, Landsize, BuildingArea, dan EffAge. 

```{r}
table(melb2$Type)
```
```{r}
table(melb2$Car)
```
```{r}
table(melb2$CouncilArea)
```
```{r}
table(melb2$Regionname)
```
```{r}
summary(melb2$ID)
```
```{r}
summary(melb2$Rooms)
```
```{r}
summary(melb2$Distance)
```
```{r}
summary(melb2$Landsize)
```
```{r}
summary(melb2$BuildingArea)
```
```{r}
summary(melb2$EffAge)
```

Berikut adalah summary dari semua independent variable yang akan digunakan di dalam projek ini.

```{r}
melb2 = melb2[melb2$Landsize != 0,]
melb2 = melb2[melb2$BuildingArea != 0,]
```

Karena tidak mungkin terdapat Landsize dan Building Area yang 0, maka dengan function di atas akan dihapuskan apabila ada data yang memiliki nilai 0 dan disimpan kembali di dalam melb2. Sekarang, akan kita lihat korelasi masing-masing independent variable yang numerical dengan dependent variable.

```{r}
cor(melb2$Price, melb2$ID)
```

Dapat dilihat bahwa, hubungan antara Price dan ID menghasilkan nilai yang 
negatif yang artinya variable ID tidak terlalu signifikan bagi variable Price.

```{r}
cor(melb2$Price, melb2$Rooms)
```

Dapat dilihat bahwa, hubungan antara Price dan Rooms menghasilkan nilai yang cukup besar yang artinya variable Rooms memiliki korelasi yang moderate bagi variable Price. 

```{r}
cor(melb2$Price, melb2$Distance)
```

Dapat dilihat bahwa, hubungan antara Price dan Distance menghasilkan nilai yang negatif yang artinya variable Distance tidak terlalu signifikan bagi variable Price.

```{r}
cor(melb2$Price, melb2$Landsize)
```

Dapat dilihat bahwa, hubungan antara Price dan Landsize menghasilkan nilai yang negatif yang artinya variable Landsize tidak terlalu signifikan bagi variable Price.

```{r}
cor(melb2$Price, melb2$BuildingArea)
```

Dapat dilihat bahwa, hubungan antara Price dan BuildingArea menghasilkan nilai yang cukup besar yang artinya variable BuildingArea memiliki korelasi yang moderate bagi variable Price. 

```{r}
cor(melb2$Price, melb2$EffAge)
```

Dapat dilihat bahwa, hubungan antara Price dan EffAge menghasilkan nilai korelasi yang kecil yang artinya variable RMTOT memiliki korelasi yang lemah bagi variable Price. 

Berikutnya, dapat kita bandingkan hasil korelasi yang sudah didapatkan dengan plot-plot untuk independent variable.

```{r}
ggarrange(ggplot(melb2) + geom_boxplot(aes(Type, Price)),
          ggplot(melb2) + geom_boxplot(aes(Car, Price)),
          ggplot(melb2) + geom_boxplot(aes(CouncilArea, Price)),
          ggplot(melb2) + geom_boxplot(aes(Regionname, Price)))
ggarrange(ggplot(melb2) + geom_point(aes(ID, Price)),
          ggplot(melb2) + geom_point(aes(Rooms, Price)), 
          ggplot(melb2) + geom_point(aes(Distance, Price)))
ggarrange(ggplot(melb2) + geom_point(aes(Landsize, Price)),
          ggplot(melb2) + geom_point(aes(BuildingArea, Price)),
          ggplot(melb2) + geom_point(aes(EffAge, Price)))
```

Dari plot, dapat dilihat juga bahwa terjadi kesamaan dengan hasil korelasi. Seperti Landsize dengan Price, yang menunjukkan hampir tidak adanya korelasi antara kedua variable. Sedangkan untuk Type dengan Price, dapat kita lihat bahwa Price ikut menurun sesuai dengan Type nya. Yang masih bisa dilihat juga outliers yang masih terdapat di masing-masing independent variable seperti pada plot Type dengan Price, dapat terlihat outliers pada Type 'u'.

# Clustering

Clustering merupakan metode pengelompokkan data yang merupakan bagian dari Data Mining. Pengelompokkan ini dilakukan dengan membagi grup yang terdiri dari data-data atau objek abstrak ke dalam kelas atau grup yang terdiri dari data-data yang sama.

Pada projek ini, kita akan mencoba untuk mensimulasikan clustering pada data melb2 dengan 3 cluster.

```{r}
melb2$Type = as.factor(melb2$Type)
melb2$CouncilArea = as.factor(melb2$CouncilArea)
melb2$Regionname = as.factor(melb2$Regionname)
melb2$Car = as.factor(melb2$Car)
melb2$Type = as.numeric(melb2$Type)
melb2$CouncilArea = as.numeric(melb2$CouncilArea)
melb2$Regionname = as.numeric(melb2$Regionname)
melb2$Car = as.numeric(melb2$Car)
```

Sebelum dapat memulai clustering, terdapat beberapa data yang harus diubah menjadi numerical menggunakan function as.factor(). dan as.numeric(). Data-data ini akan berubah menjadi angka dan ini dapat dilihat dibawah dari hasil function summary().

```{r}
table(is.na(melb2))
melb2 = na.omit(melb2)

summary(melb2)
```


```{r}
clustermod = kmeans(na.omit(melb2), 3, nstart = 100)
clustermod
```

```{r}
plot(melb2$Price, melb2$ID, col = clustermod$cluster)
plot(melb2$Price, melb2$EffAge, col = clustermod$cluster)
```

```{r}
kmax = 15
wss = sapply(1:kmax, function(k){kmeans(na.omit(melb2), k, nstart = 100, iter.max = 15)$tot.withinss})
plot(1:kmax, wss, type = "b", pch = 19, xlab = "Number of Cluster", ylab = "sum of squares", col = "magenta")
```

```{r}
melb2$cluster = clustermod$cluster
summary(melb2)
```


# Linear Based Model / Additive Model

Linear model adalah suatu metode untuk memvisualisasikan variable respon atau dependent variable dalam hal kombinasi linier variable prediktor. Dalam Linear Model projek ini tersedia pilihan: Generalized Linear Modeling (GLM) atau Generalized Additive Model (GAM).

## Data Training dan Testing

Pada bagian clustering sebelumnya, terdapat beberapa data categorical yang harus diubah menjadi numerical agar dapat diolah. Untuk simulasi model Linear atau Additive, akan diubah kembali menjadi categorical menggunakan function factor(). Dapat dilihat summary dari data melb2 dengan function summary() setelah data categorical diubah kembali.

```{r}
melb2$Type = factor(melb2$Type)
melb2$CouncilArea = factor(melb2$CouncilArea)
melb2$Regionname = factor(melb2$Regionname)
melb2$Car = factor(melb2$Car)
summary(melb2)
```

Sebelum dapat memulai simulasi model, data akan dibagi dulu ke dalam data tDengan function melb2[] menggunakan p, dapat kita bagi data melb masuk ke dalam train.d dan test.d. Jumlah yang dihasilkan untuk kedua set data sudah sesuai mengikuti rasio 80:20, hal ini dapat dilihat dari hasil function dim() dibawah. Selanjutnya kita bisa memulai mensimulasikan model nya.raining dan data testing menggunakan fungsi createDataPartition() dari library(caret) dengan rasio 80:20. Data Splitting ini dilakukan berdasarkan variable Type sebagai salah satu independent variable yang bersifat categorical yang akan disimulasikan modelnya.

```{r}
set.seed(136)
p = createDataPartition(melb2$Type, p = 0.8, list = FALSE)

train.d = melb2[p, ]
test.d = melb2[-p, ]
```



```{r}
dim(train.d)
dim(test.d)

```


## Model Simulation

Model yang akan disimulasikan dalam projek ini adalah GLM atau Generalized Linear Model. GLM adalah generalisasi yang fleksibel dari regresi linier biasa. GLM menggeneneralisasikan regresi linier dengan membiarkan model linier dikaitkan dengan suatu dependent variable dan membiarkan besarnya varian dari setiap independent variable yang menjadi fungsi dari nilai prediksi hasilnya.

GLM Family yang disediakan oleh Rstudio adalah:

- Binomial
- Gaussian
- Gamma
- Inverse Gaussian
- Poisson
- Quasi

Yang akan kita gunakan pada projek ini adalah family Gaussian dengan data yang disimulasikan adalah data train.d. Kita gunakan family Gaussian karena variable yang dihasilkan bersifat continuous.

```{r}
set.seed(29)
glm.mod = glm(log(Price)~., family = "gaussian", data = train.d)
summary(glm.mod)
```

Dari glm.mod, didapatkan nilai AIC -497.78 yang akan digunakan untuk dibandingkan dengan model satunya dengan data train.d yang sudah dihapus outlier nya.

Berikut adalah plot untuk membantu visualisasi hasil glm.mod yang sudah di dapatkan.

```{r}
plot(glm.mod$fitted.values, glm.mod$residuals)
```

```{r}
plot(glm.mod$fitted.values, glm.mod$y)
```

Berikutnya ini juga merupakan plot visualisasi glm.mod dengan garis abline yang merepresentasikan nilai aslinya.

```{r}
ggplot() + 
 geom_point(aes(x = glm.mod$fitted.values, y = log(train.d$Price))) +
 geom_abline(aes(intercept = 0, slope = 1), colour = "sky blue") + 
 ggtitle("log Price vs. Prediction - Training Data With Outliers") +
 theme(plot.title = element_text(hjust = 0.5)) + 
 labs(x = "Train Data Prediction", y = "Melb2 Price")
```


```{r}
residu = data.frame(x = rstandard(glm.mod))
pred = glm.mod$fitted.values
```

```{r}
ggplot() + 
 geom_point(aes(x = pred, y = residu$x)) +
 geom_abline(aes(intercept = 0, slope = 0), colour = "sky blue") +
 ggtitle("Residual vs. Prediction - Training Data with Outliers") +
 theme(plot.title = element_text(hjust = 0.5)) +
 labs(x = "Prediction on Train Data", y = "Residual")
```

```{r}
bin1 = which(abs(residu) > 3)
if(length(bin1)>0){
 train.outliers1 = train.d
 train.outliers1$outliers = 0
 train.outliers1$outliers[bin1] = 1
 train.outliers1$pred = glm.mod$fitted.values
 train.outliers1$pred.dollar = exp(train.outliers1$pred)
 train.d2 = train.d[-bin1,]
} else{
 train.d2 = train.d
}
```

```{r}
glm.modf = glm(log(Price)~., family = "gaussian", data = train.d2)
summary(glm.modf)
```

```{r}
residuf = data.frame(x = rstandard(glm.modf))
predf = glm.modf$fitted.values
```

```{r}
ggplot() + 
 geom_point(aes(x = predf, y = log(train.d2$Price))) +
 geom_abline(aes(intercept = 0, slope = 1), colour = "purple") +
 ggtitle("Sales Price vs. Prediction - Train Data without Outliers") +
 theme(plot.title = element_text(hjust = 0.5)) +
 labs(x = "Prediction on Train Data", y = "Residuals")
```

```{r}
ggplot() +
 geom_point(aes(x = predf, y = residuf$x)) + 
 geom_abline(aes(intercept = 0, slope = 0), colour = "purple") +
 ggtitle("Residual vs. Prediction - Training Data without Outliers") +
 theme(plot.title = element_text(hjust = 0.5)) +
 labs(x = "Prediction on Train Data", y = "Residuals")
```

```{r}
vif(glm.modf)
```

```{r}
comp = function(pred, obs){
 n = length(obs)
 rsq = cor(pred, obs)^2
 mse = sum((pred - obs)^2)/ n
 semse = sd((pred - obs)^2) / sqrt(n)
 rmse = sqrt(mse)
 se = sd(pred - obs) / sqrt(n)
 mae = sum(abs(pred - obs)) / n
 mape = sum(abs(pred - obs) / obs) / (n*100)
return(list("n" = n, "R2" = rsq, "MSE" = mse, "SEMSE" = semse, "RMSE" = rmse, "SE" = se, "MAE" = mae, "MAPE" = mape))
}
comp(glm.modf$fitted.values, glm.modf$y)
```

```{r}
test.d$prediction = predict(glm.modf, newdata = test.d, type = "response")
ggplot() + 
  geom_point(aes(x = test.d$prediction, y = log(test.d$Price))) +
  geom_abline(aes(intercept = 0, slope = 1), colour = "hot pink") +
  ggtitle("Log SalePrice vs Prediction - Testing Set") +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = "Prediction on Test Data", y ="Log Sales Price")

comp(test.d$prediction, test.d$Price)
```

# Regression Tree

## Data Training dan Data Testing

```{r}
set.seed(127)
n.folds = 10

melb2$folds = createFolds(melb2$Price, k = n.folds, list = FALSE, returnTrain = FALSE)

test.sd = melb2[melb2$folds == 10, ]
train.val.sd = melb2[melb2$folds != 10, ]

table(is.na(train.val.sd))
train.val.sd = na.omit(train.val.sd)
table(is.na(test.sd))
test.sd = na.omit(test.sd)
```

```{r}
train.val.sd$folds2 = createFolds(train.val.sd$Price, k = n.folds, list = FALSE, returnTrain = FALSE)

table(is.na(train.val.sd$folds2))
train.val.sd$folds2 = na.omit(train.val.sd$folds2)
```

## Tree Simulation

```{r}
comp = function(pred, obs){
  n = length(obs)
  rsq = cor(pred,obs)^2
  mse = sum((pred - obs)^2)/n
  semse = sd((pred - obs)^2) / sqrt(n)
  rmse = sqrt(mse)
  se = sd(pred-obs) / sqrt(n)
  mae = sum(abs(pred-obs))/n
  mape = sum(abs(pred-obs)/obs)/n*100
  return(list("n"=n,"R2"=rsq,"MSE"=mse,"SEMSE"=semse,"RMSE"=rmse,"SE"=se,"MAE"=mae,"MAPE"=mape))
}
```

```{r}
ntree = c(1, 3, 5, 7, 10, 15, 20)
mtry = c(11/3) #10 independent variable + cluster

MAPE = NULL
MAPE.ave = matrix(, nrow = length(ntree), ncol = length(mtry))
rownames(MAPE.ave) = ntree
colnames(MAPE.ave) = mtry

for(j in 1:length(ntree)){
  t = ntree[j]
  for(k in 1:length(mtry)){
    m = mtry[k]
    for(i in 1:10){
      train.set = train.val.sd[train.val.sd$folds2 != i, ]
      val.sd = train.val.sd[train.val.sd$folds2 == i, ]
      
      
      rf = randomForest(formula = Price~., data = train.set, 
                        mtry = m, ntree = t)
      
      val.sd$pred = predict(rf, val.sd)
      MAPE[i] = comp(val.sd$pred, val.sd$Price)$MAPE
    }
    MAPE.ave[j, k] = mean(MAPE)
  }
}
MAPE.ave
opt.ntree = ntree[which.min(MAPE.ave)]
opt.ntree
```

```{r}
rf.final = randomForest(formula = Price~., data = train.val.sd, 
                        mtry = 11/3, ntree = opt.ntree)
test.sd$pred = predict(rf.final, newdata = test.sd) # untuk ini harus di run ulang train test nya itu (line 431)
rf.final
comp(test.sd$pred, test.sd$Price)
plot(rf.final)
```